{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                  GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
    "\n",
    "from transfer_learning_conv_ai.utils import get_dataset, make_logdir\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    dataset_path=''\n",
    "    dataset_cache='./dataset_cache'\n",
    "    model_checkpoint='openai-gpt'\n",
    "    output_dir = ''\n",
    "    num_candidates=2\n",
    "    max_history=2\n",
    "    per_gpu_train_batch_size=4\n",
    "    per_gpu_eval_batch_size=1\n",
    "    gradient_accumulation_steps=8\n",
    "    lr=6.25e-5\n",
    "    lm_coef=1.0\n",
    "    mc_coef=1.0\n",
    "    max_norm=1.0\n",
    "    #n_epochs=3\n",
    "    personality_permutations=1\n",
    "    eval_before_start=True\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    fp16='O1'\n",
    "    local_rank=-1\n",
    "    distributed = (local_rank != -1)\n",
    "    max_step = -1\n",
    "    num_train_epochs = 3\n",
    "    evaluate_during_training = True\n",
    "    logging_steps = 8\n",
    "    n_gpu=1\n",
    "    max_steps = -1\n",
    "    tpu = False\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "args=args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {\"bos_token\": \"<bos>\", \"eos_token\": \"<eos>\", \n",
    "                  \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"],\n",
    "                  \"pad_token\": \"<pad>\"}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Running process %d\", args.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n",
    "logger.info(\"Arguments: %s\", pformat(args))\n",
    "    \n",
    "# def average_distributed_scalar(scalar, args):\n",
    "#     \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "#     if args.local_rank == -1:\n",
    "#         return scalar\n",
    "#     scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n",
    "#     torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "#     return scalar_t.item()\n",
    "\n",
    "\n",
    "# num_added_toks = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
    "tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer # cant use Autotokenizer because checkpoint could be a Path\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "model = model_class.from_pretrained(args.model_checkpoint)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "        \n",
    "add_special_tokens_(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "# if args.fp16:\n",
    "#     from apex import amp  # Apex is only required if we use fp16 training\n",
    "#     model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16)\n",
    "\n",
    "# from transformers import cached_path\n",
    "# import json\n",
    "\n",
    "# logger.info(\"Prepare datasets\")\n",
    "# #train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)\n",
    "\n",
    "# PERSONACHAT_URL = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
    "\n",
    "# dataset_path = PERSONACHAT_URL\n",
    "\n",
    "# personachat_file = cached_path(dataset_path)\n",
    "\n",
    "# with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     dataset = json.loads(f.read())\n",
    "\n",
    "# def tokenize(obj):\n",
    "#     if isinstance(obj, str):\n",
    "#         return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "#     if isinstance(obj, dict):\n",
    "#         return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "#     return list(tokenize(o) for o in obj)\n",
    "\n",
    "# dataset = tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "\n",
    "dataset_cache = args.dataset_cache\n",
    "dataset_cache = dataset_cache + '_' + type(tokenizer).__name__ \n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    #[[bos+persona], [history], [reply+eos]]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels: #if the current candidate is lm_labels, [-100]*[len(persona)+len(history)+1(speaker2)]+current candidate\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n",
    "\n",
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset\n",
    "\n",
    "def get_data_loaders(args, tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = torch.load(dataset_cache)#get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
    "\n",
    "    logger.info(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"]) #n_candidates are same for all 17878 dialogs\n",
    "        if args.num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(args.num_candidates, num_candidates) #min(2,20)\n",
    "        for dialog in dataset: #17878 dialogs\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(args.personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]: #7ä¸ªutterances in the first dialog\n",
    "                    history = utterance[\"history\"][-(2*args.max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1) #the last sentence in candidate is the correct response\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        #instance[\"input_ids\"] of length of the sequence: bos+persona+history+candiate+eos\n",
    "                        #, instance[\"token_type_ids\"], instance[\"mc_token_ids\"], instance[\"lm_labels\"] \n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                        #datasets['train']['input_ids'] of [[c1 in u1],[c2 in u1],..,[c2 in u7]] 14 sublists [n_candidate* # of utterances in a dialog] e.g. [2*7]\n",
    "                        # the first is the sequence with wrong candidate, second is the sequence with correct candidate\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1) #7\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates #an int =2\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name]) #1 dialog->7 or 6 or 8 untterences == 17878 dialogs -> sum(utterances in each dialog) =131438\n",
    "            #np.sum([len(dialog['utterances']) for dialog in personachat['train']])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    logger.info(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "#     train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
    "#     valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if args.distributed else None\n",
    "#     train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(not args.distributed))\n",
    "#     valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\n",
    "\n",
    "    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape)) #torch.Size([131438 utterences, 2 candidates, 282 max_lens])\n",
    "    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_dataset, valid_dataset#, train_loader, valid_loader, train_sampler, valid_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = get_data_loaders(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, valid_dataset, model, tokenizer):\n",
    "    \n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.num_train_epochs * len(train_dataloader), 0.0)])\n",
    "    \n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    \n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    \n",
    "    global_step = 0\n",
    "    metrics = {\"nll\": 10000.0,\n",
    "           \"accuracy\": 0.0}\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    #train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    \n",
    "    for _ in range(args.num_train_epochs): #3\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]) #131438\n",
    "        for step, batch in enumerate(epoch_iterator): #4 utterances\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "            (lm_loss), (mc_loss), *_ = model(\n",
    "                input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "                mc_labels=mc_labels, lm_labels=lm_labels\n",
    "            )\n",
    "            loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef) / args.gradient_accumulation_steps\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                logger.info(\"Loss for {} is {}\".format(step, loss))\n",
    "                \n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "                \n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0 and not args.tpu:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                \n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (step+1) % args.logging_steps == 0:\n",
    "                    \n",
    "                    if args.local_rank == -1 and args.evaluate_during_training:\n",
    "                        \n",
    "                        metrics = evaluate(args, model, valid_dataset, metrics, tokenizer)\n",
    "            \n",
    "                global_step+=1\n",
    "            \n",
    "            if args.max_steps>0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "    \n",
    "    return tr_loss/global_step, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(args, model, valid_dataset, metrics, tokenizer, prefix=''):\n",
    "    \n",
    "    eval_outputs_dirs = make_logdir(args.model_checkpoint)\n",
    "    \n",
    "    if not os.path.exists(eval_outputs_dirs) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_outputs_dirs)\n",
    "        \n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(valid_dataset) if args.local_rank == -1 else DistributedSampler(valid_dataset)\n",
    "    eval_dataloader = DataLoader(valid_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    \n",
    "        # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(valid_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    \n",
    "    nlls = None\n",
    "    accs = None\n",
    "    eval_epoch_iterator = tqdm(eval_dataloader, desc=\"Evaluating\")\n",
    "    for _, batch in enumerate(eval_epoch_iterator):\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "            \n",
    "            lm_logits, mc_logits, *_ = model(\n",
    "                input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "            )\n",
    "            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "            \n",
    "            x = ((lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels))\n",
    "            nll = torch.nn.CrossEntropyLoss(ignore_index=-100)(x[0][0], x[1][0]).detach().cpu().numpy()#Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0]))\n",
    "            acc = torch.sum((torch.max(x[0][1], 1)[1] == x[1][1]).int()).detach().cpu().numpy().mean()#Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))\n",
    "        \n",
    "        if nlls is None:\n",
    "            nlls = nll\n",
    "            accs = acc\n",
    "        else:\n",
    "            nlls = np.append(nlls, nll)\n",
    "            accs = np.append(accs, acc)\n",
    "    \n",
    "    nlls_mean = np.mean(nlls)\n",
    "    accs_mean = np.mean(accs)\n",
    "    \n",
    "    if accs_mean>metrics['accuracy'] and nlls_mean<metrics['nll']:\n",
    "        logger.info(\"***** New high accuracy and nll! {} {}*****\".format(accs_mean, nlls_mean))\n",
    "        metrics.update({'nll': nlls_mean, 'accuracy': accs_mean})\n",
    "        output_eval_file = os.path.join(eval_outputs_dirs, prefix, \"eval_results.txt\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_loss, eval_results = train(args, train_dataset, valid_dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JupyterPy2",
   "language": "python",
   "name": "ipykernel_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
