{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(40483, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = {\"bos_token\": \"<bos>\", \"eos_token\": \"<eos>\", \n",
    "                  \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"],\n",
    "                  \"pad_token\": \"<pad>\"}\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "#model.set_num_special_tokens(len(SPECIAL_TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "def build_inputs(persona, history, reply):\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                    for i, s in enumerate(sequence[1:])]\n",
    "\n",
    "    '''\n",
    "    [['<bos>',\n",
    "      'i',\n",
    "      'like',\n",
    "      'playing',\n",
    "      'football',\n",
    "      '.',\n",
    "      'i',\n",
    "      'am',\n",
    "      'from',\n",
    "      'NYC',\n",
    "      '.'],\n",
    "     ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],\n",
    "     ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],\n",
    "     ['<speaker1>', 'great', 'to', 'hear', '<eos>']]\n",
    "    '''\n",
    "    words = list(chain(*sequence)) #a flat list with special tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                    for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))\n",
    "    \n",
    "    return words, segments, position, sequence\n",
    "\n",
    "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
    "# >>> print(sequence)  # Our inputs looks like this:\n",
    "# [['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.'],\n",
    "#  ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],\n",
    "#  ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],\n",
    "#  ['<speaker1>', 'great', 'to', 'hear', '<eos>']]\n",
    "#>>> print(words)\n",
    "#['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.', '<speaker1>', 'hello', 'how', 'are', 'you', '?', '<speaker2>', 'i', 'am', 'fine', 'thanks', '.', '<speaker1>', 'great', 'to', 'hear', '<eos>']\n",
    "#>>> print(segments)\n",
    "#['<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>']\n",
    "#>>> print(position)\n",
    "#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.convert_tokens_to_ids(words)\n",
    "segments = tokenizer.convert_tokens_to_ids(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "distractor = [\"sorry\", \"to\", \"hear\", \"that\"]\n",
    "\n",
    "words_distractor, segments_distractor, _, _ = build_inputs(persona, history, distractor)\n",
    "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
    "segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
    "\n",
    "lm_targets = ([-100] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-100] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "#[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5201, 571, 863, 40479]\n",
    "lm_distractor = [-100] * len(words_distractor)\n",
    "\n",
    "last_token = len(words) - 1 #<eos>\n",
    "last_token_distractor = len(words_distractor) - 1\n",
    "\n",
    "padding_length = max(len(words), len(words_distractor))\n",
    "\n",
    "def pad(x, padding):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "(words, words_distractor,\n",
    " segments, segments_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                   for x in (words, words_distractor,\n",
    "                                             segments, segments_distractor)]\n",
    "\n",
    "(lm_targets, lm_distractor) = [pad(x, -100) for x in (lm_targets, lm_distractor)]\n",
    " \n",
    "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long) #torch.Size([1, 2, 29])\n",
    "\n",
    "token_type_ids = torch.tensor([[segments, segments_distractor]], dtype=torch.long)\n",
    "\n",
    "mc_token_ids = torch.tensor([[last_token, last_token_distractor]], dtype=torch.long) #tensor([[28, 28]])\n",
    "# Language modeling labels\n",
    "lm_labels = torch.tensor([[lm_targets, lm_distractor]], dtype=torch.long)\n",
    "# Next-sentence prediction labels\n",
    "mc_labels = torch.tensor([0], dtype=torch.long)  # Gold reply is 1st (index 0)\n",
    "\n",
    "lm_logits, mc_logits, *_ = model(input_ids=input_ids, \n",
    "                         mc_token_ids=mc_token_ids, \n",
    "#                          lm_labels=lm_labels, \n",
    "#                          mc_labels=mc_labels, \n",
    "                         token_type_ids=token_type_ids)\n",
    "# mc_token_ids, lm_labels.shape, mc_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([0]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(mc_logits, 1)[1], mc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_coef = 2.0\n",
    "mc_coef = 1.0\n",
    "total_loss = lm_loss * lm_coef + mc_loss * mc_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                  GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
    "\n",
    "from transfer_learning_conv_ai.utils import get_dataset, make_logdir\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    dataset_path=''\n",
    "    dataset_cache='./dataset_cache'\n",
    "    model_checkpoint='openai-gpt'\n",
    "    output_dir = ''\n",
    "    num_candidates=2\n",
    "    max_history=2\n",
    "    per_gpu_train_batch_size=4\n",
    "    per_gpu_eval_batch_size=1\n",
    "    gradient_accumulation_steps=8\n",
    "    lr=6.25e-5\n",
    "    lm_coef=1.0\n",
    "    mc_coef=1.0\n",
    "    max_norm=1.0\n",
    "    #n_epochs=3\n",
    "    personality_permutations=1\n",
    "    eval_before_start=True\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    fp16='O1'\n",
    "    local_rank=-1\n",
    "    distributed = (local_rank != -1)\n",
    "    max_step = -1\n",
    "    num_train_epochs = 3\n",
    "    evaluate_during_training = True\n",
    "    logging_steps = 8\n",
    "    n_gpu=1\n",
    "    max_steps = -1\n",
    "    tpu = False\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "args=args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Running process -1\n",
      "INFO:__main__:Arguments: <class '__main__.args'>\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {\"bos_token\": \"<bos>\", \"eos_token\": \"<eos>\", \n",
    "                  \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"],\n",
    "                  \"pad_token\": \"<pad>\"}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Running process %d\", args.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n",
    "logger.info(\"Arguments: %s\", pformat(args))\n",
    "    \n",
    "# def average_distributed_scalar(scalar, args):\n",
    "#     \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "#     if args.local_rank == -1:\n",
    "#         return scalar\n",
    "#     scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n",
    "#     torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "#     return scalar_t.item()\n",
    "\n",
    "\n",
    "# num_added_toks = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Prepare tokenizer, pretrained model and optimizer.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /home/ubuntu/.cache/torch/transformers/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /home/ubuntu/.cache/torch/transformers/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /home/ubuntu/.cache/torch/transformers/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.bd0797be126548711309ad2174d2afb16e3c37e891707667603d85e35a4ad001\n",
      "INFO:transformers.configuration_utils:Model config OpenAIGPTConfig {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIGPTDoubleHeadsModel(\n",
       "  (transformer): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40478, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=40478, bias=False)\n",
       "  (multiple_choice_head): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (activation): Identity()\n",
       "    (first_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (last_dropout): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
    "tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer # cant use Autotokenizer because checkpoint could be a Path\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "model = model_class.from_pretrained(args.model_checkpoint)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <speaker1> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <speaker2> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n"
     ]
    }
   ],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "        \n",
    "add_special_tokens_(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "# if args.fp16:\n",
    "#     from apex import amp  # Apex is only required if we use fp16 training\n",
    "#     model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16)\n",
    "\n",
    "# from transformers import cached_path\n",
    "# import json\n",
    "\n",
    "# logger.info(\"Prepare datasets\")\n",
    "# #train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)\n",
    "\n",
    "# PERSONACHAT_URL = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
    "\n",
    "# dataset_path = PERSONACHAT_URL\n",
    "\n",
    "# personachat_file = cached_path(dataset_path)\n",
    "\n",
    "# with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     dataset = json.loads(f.read())\n",
    "\n",
    "# def tokenize(obj):\n",
    "#     if isinstance(obj, str):\n",
    "#         return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "#     if isinstance(obj, dict):\n",
    "#         return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "#     return list(tokenize(o) for o in obj)\n",
    "\n",
    "# dataset = tokenize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "\n",
    "dataset_cache = args.dataset_cache\n",
    "dataset_cache = dataset_cache + '_' + type(tokenizer).__name__ \n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    #[[bos+persona], [history], [reply+eos]]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels: #if the current candidate is lm_labels, [-100]*[len(persona)+len(history)+1(speaker2)]+current candidate\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n",
    "\n",
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset\n",
    "\n",
    "def get_data_loaders(args, tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = torch.load(dataset_cache)#get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
    "\n",
    "    logger.info(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"]) #n_candidates are same for all 17878 dialogs\n",
    "        if args.num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(args.num_candidates, num_candidates) #min(2,20)\n",
    "        for dialog in dataset: #17878 dialogs\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(args.personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]: #7个utterances in the first dialog\n",
    "                    history = utterance[\"history\"][-(2*args.max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1) #the last sentence in candidate is the correct response\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        #instance[\"input_ids\"] of length of the sequence: bos+persona+history+candiate+eos\n",
    "                        #, instance[\"token_type_ids\"], instance[\"mc_token_ids\"], instance[\"lm_labels\"] \n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                        #datasets['train']['input_ids'] of [[c1 in u1],[c2 in u1],..,[c2 in u7]] 14 sublists [n_candidate* # of utterances in a dialog] e.g. [2*7]\n",
    "                        # the first is the sequence with wrong candidate, second is the sequence with correct candidate\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1) #7\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates #an int =2\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name]) #1 dialog->7 or 6 or 8 untterences == 17878 dialogs -> sum(utterances in each dialog) =131438\n",
    "            #np.sum([len(dialog['utterances']) for dialog in personachat['train']])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    logger.info(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "#     train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
    "#     valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if args.distributed else None\n",
    "#     train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(not args.distributed))\n",
    "#     valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\n",
    "\n",
    "    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape)) #torch.Size([131438 utterences, 2 candidates, 282 max_lens])\n",
    "    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_dataset, valid_dataset#, train_loader, valid_loader, train_sampler, valid_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Build inputs and labels\n",
      "INFO:__main__:Pad inputs and convert to Tensor\n",
      "INFO:__main__:Build train and validation dataloaders\n",
      "INFO:__main__:Train dataset (Batch, Candidates, Seq length): torch.Size([131438, 2, 282])\n",
      "INFO:__main__:Valid dataset (Batch, Candidates, Seq length): torch.Size([7801, 20, 179])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = get_data_loaders(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "# train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "# args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "# eval_sampler = SequentialSampler(valid_dataset) if args.local_rank == -1 else DistributedSampler(valid_dataset)\n",
    "# eval_dataloader = DataLoader(valid_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    \n",
    "# train_batch = next(iter(train_dataloader))\n",
    "# eval_batch = next(iter(eval_dataloader))\n",
    "\n",
    "# batch = tuple(t.to(args.device) for t in eval_batch)\n",
    "# input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "\n",
    "# lm_logits, mc_logits, *_ = model(\n",
    "#             input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "#         )\n",
    "\n",
    "# lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "# lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "\n",
    "# x = ((lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels))\n",
    "\n",
    "# torch.nn.CrossEntropyLoss(ignore_index=-100)(x[0][0], x[1][0]), torch.mean((torch.max(x[0][1], 1)[1] == x[1][1]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, valid_dataset, model, tokenizer):\n",
    "    \n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.num_train_epochs * len(train_dataloader), 0.0)])\n",
    "    \n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    \n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    \n",
    "    global_step = 0\n",
    "    metrics = {\"nll\": 10000.0,\n",
    "           \"accuracy\": 0.0}\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    #train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    \n",
    "    for _ in range(args.num_train_epochs): #3\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]) #131438\n",
    "        for step, batch in enumerate(epoch_iterator): #4 utterances\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "            (lm_loss), (mc_loss), *_ = model(\n",
    "                input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "                mc_labels=mc_labels, lm_labels=lm_labels\n",
    "            )\n",
    "            loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef) / args.gradient_accumulation_steps\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                logger.info(\"Loss for {} is {}\".format(step, loss))\n",
    "                \n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "                \n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0 and not args.tpu:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                \n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (step+1) % args.logging_steps == 0:\n",
    "                    \n",
    "                    if args.local_rank == -1 and args.evaluate_during_training:\n",
    "                        \n",
    "                        metrics = evaluate(args, model, valid_dataset, metrics, tokenizer)\n",
    "            \n",
    "                global_step+=1\n",
    "            \n",
    "            if args.max_steps>0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "    \n",
    "    return tr_loss/global_step, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate(args, model, valid_dataset, metrics, tokenizer, prefix=''):\n",
    "    \n",
    "    eval_outputs_dirs = make_logdir(args.model_checkpoint)\n",
    "    \n",
    "    if not os.path.exists(eval_outputs_dirs) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_outputs_dirs)\n",
    "        \n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(valid_dataset) if args.local_rank == -1 else DistributedSampler(valid_dataset)\n",
    "    eval_dataloader = DataLoader(valid_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    \n",
    "        # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(valid_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    \n",
    "    nlls = None\n",
    "    accs = None\n",
    "    eval_epoch_iterator = tqdm(eval_dataloader, desc=\"Evaluating\")\n",
    "    for _, batch in enumerate(eval_epoch_iterator):\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "            input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "            \n",
    "            lm_logits, mc_logits, *_ = model(\n",
    "                input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "            )\n",
    "            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "            \n",
    "            x = ((lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels))\n",
    "            nll = torch.nn.CrossEntropyLoss(ignore_index=-100)(x[0][0], x[1][0]).detach().cpu().numpy()#Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0]))\n",
    "            acc = torch.sum((torch.max(x[0][1], 1)[1] == x[1][1]).int()).detach().cpu().numpy().mean()#Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))\n",
    "        \n",
    "        if nlls is None:\n",
    "            nlls = nll\n",
    "            accs = acc\n",
    "        else:\n",
    "            nlls = np.append(nlls, nll)\n",
    "            accs = np.append(accs, acc)\n",
    "    \n",
    "    nlls_mean = np.mean(nlls)\n",
    "    accs_mean = np.mean(accs)\n",
    "    \n",
    "    if accs_mean>metrics['accuracy'] and nlls_mean<metrics['nll']:\n",
    "        logger.info(\"***** New high accuracy and nll! {} {}*****\".format(accs_mean, nlls_mean))\n",
    "        metrics.update({'nll': nlls_mean, 'accuracy': accs_mean})\n",
    "        output_eval_file = os.path.join(eval_outputs_dirs, prefix, \"eval_results.txt\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 131438\n",
      "INFO:__main__:  Num Epochs = 3\n",
      "INFO:__main__:  Instantaneous batch size per GPU = 4\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "INFO:__main__:  Gradient Accumulation steps = 8\n",
      "INFO:__main__:  Total optimization steps = 12321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 0/32860 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loss for 0 is 0.5290927886962891\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 1/32860 [00:02<18:59:26,  2.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 2/32860 [00:03<18:28:32,  2.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 3/32860 [00:05<18:08:01,  1.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 4/32860 [00:07<17:55:25,  1.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 5/32860 [00:09<17:45:50,  1.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 6/32860 [00:11<17:38:44,  1.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 7/32860 [00:13<17:35:14,  1.93s/it]\u001b[A\u001b[A\u001b[A\u001b[AINFO:__main__:***** Running evaluation  *****\n",
      "INFO:__main__:  Num examples = 5\n",
      "INFO:__main__:  Batch size = 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e4517441467b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-0077dbf986ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, valid_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_during_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mglobal_step\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b8e5ae90b8ac>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, valid_dataset, metrics, tokenizer, prefix)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             lm_logits, mc_logits, *_ = model(\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "tr_loss, eval_results = train(args, train_dataset, valid_dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(engine, batch):\n",
    "    model.train()\n",
    "    batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "    input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "    (lm_loss), (mc_loss), *_ = model(\n",
    "        input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        mc_labels=mc_labels, lm_labels=lm_labels\n",
    "    )\n",
    "    loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef) / args.gradient_accumulation_steps\n",
    "    if args.fp16:\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
    "    if engine.state.iteration % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "trainer = Engine(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "        logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "        # if we dont send labels to model, it doesnt return losses\n",
    "        lm_logits, mc_logits, *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        )\n",
    "        lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "        lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "        return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)\n",
    "evaluator = Engine(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "if args.n_epochs < 1:\n",
    "    trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "if args.eval_before_start:\n",
    "    trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "\n",
    "if args.distributed:\n",
    "    trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n",
    "    evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.RemovableEventHandle at 0x7f958048a278>"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linearly decrease the learning rate from lr to zero\n",
    "scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distributed_scalar(scalar, args):\n",
    "    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "    if args.local_rank == -1:\n",
    "        return scalar\n",
    "    scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n",
    "    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "    return scalar_t.item()\n",
    "\n",
    "# Prepare metrics - note how we compute distributed metrics\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "           \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args), #applying average_distributed_scalar to metrics[\"nll\"] \n",
    "                \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/handlers/checkpoint.py:369: UserWarning: Argument save_interval is deprecated and should be None. Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('runs/May22_23-18-44_ip-172-31-29-0_openai-gpt/vocab.json',\n",
       " 'runs/May22_23-18-44_ip-172-31-29-0_openai-gpt/merges.txt',\n",
       " 'runs/May22_23-18-44_ip-172-31-29-0_openai-gpt/special_tokens_map.json',\n",
       " 'runs/May22_23-18-44_ip-172-31-29-0_openai-gpt/added_tokens.json')"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbar = ProgressBar(persist=True)\n",
    "pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "log_dir = make_logdir(args.model_checkpoint)\n",
    "\n",
    "checkpoint_handler = ModelCheckpoint(log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n",
    "\n",
    "torch.save(args, log_dir + '/model_training_args.bin')\n",
    "getattr(model, 'module', model).config.to_json_file(os.path.join(log_dir, CONFIG_NAME))\n",
    "tokenizer.save_pretrained(log_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ignite.engine.engine.Engine:Engine run resuming from iteration 0, epoch 0 until 3 epochs\n",
      "INFO:ignite.engine.engine.Engine:Engine run resuming from iteration 1, epoch 1 until 1 epochs\n",
      "\n",
      "\n",
      "                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                       \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                 \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                         \n",
      "\u001b[A                                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                    \n",
      "\n",
      "  0%|          | 0/2 [124:56:25<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18878 [124:13:59<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [124:53:06<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [124:52:56<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18878 [124:13:45<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [124:52:56<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  0%|          | 0/2 [124:56:09<?, ?it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18878 [124:13:45<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "                                         \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                           \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                     \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                             \n",
      "\u001b[A                                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A                        \n",
      "\n",
      "  0%|          | 0/2 [124:56:25<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18878 [124:13:59<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [124:53:06<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [124:52:56<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18878 [124:13:45<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [124:52:56<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  0%|          | 0/2 [124:56:09<?, ?it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18878 [124:13:45<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AINFO:ignite.engine.engine.Engine:Engine run complete. Time taken 00:00:00\n",
      "ERROR:ignite.engine.engine.Engine:Current run is terminating due to exception: CUDA error: device-side assert triggered.\n",
      "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: CUDA error: device-side assert triggered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {}\n",
      "Validation: {}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-363-ac5989058f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-340-77d5a22871c0>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     (lm_loss), (mc_loss), *_ = model(\n",
      "\u001b[0;32m<ipython-input-340-77d5a22871c0>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     (lm_loss), (mc_loss), *_ = model(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=args.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
